{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhsWd-Jt3HI0",
        "outputId": "25ef80e8-bbda-44f7-f2a8-016fd2b957f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec using CBOW"
      ],
      "metadata": {
        "id": "ZIcYZr-wMlt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "moOH_QuLMhmI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FiMUPB3m2bMJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
        "import numpy as np\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from functools import partial\n",
        "from torch import optim\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading given Corpus file.\n",
        "\n",
        "Here we are taking just a subset of data in order to save some compute."
      ],
      "metadata": {
        "id": "Hfqt_OPoM8Ug"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Z7ey57yg2bMK"
      },
      "outputs": [],
      "source": [
        "filename = 'reviews_Movies_and_TV.json'\n",
        "lim = 100000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVUpjka52bMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8f7082-8386-416f-ed67-a33e72883fd4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7f7ae1c6c56a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"newfile.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reviews_Movies_and_TV.json'"
          ]
        }
      ],
      "source": [
        "with open(filename, \"r\") as f:\n",
        "    with open(\"newfile.json\", \"w\") as fp:\n",
        "        for i in range(lim):\n",
        "            fp.write(f.readline())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SQDgyz182bML"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(\"./drive/MyDrive/data/newfile.json\", lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "33QpJVuP2bML"
      },
      "outputs": [],
      "source": [
        "corpus = df[\"reviewText\"].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhapT5PX2bML",
        "outputId": "fb9333ad-356a-4389-f128-0392d9d1416c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"This has some great tips as always and is helping me to complete my Good Eats collection.  I haven't tried any of the recipes yet, but I will soon.  Sometimes it's just lovely to let Alton entertain us.\",\n",
              "       \"This is a great pastry guide.  I love how Alton's collections can break it down so baking isn't so mystical and scary.  I might even try some of these recipes some day.\",\n",
              "       \"I have to admit that I am a fan of Giada's cooking and I had great expectations when I ordered this set. They were however, crushed. While I still love Giada's cooking, this set is just a way for Food Network to make money. They really cheated with these DVD's. All they have are the video from the show, no text recipes, no link to the on line shows and no computer support. They play in Windows media player but the set does not contain the recipes. You can get more by taping the shows and then going to the web to download recipes. Another disappointment is the so so transfer quality to DVD. Perhaps I've been spoiled by HD and Tivo but the older shows I've recorded have had better playback quality than the episodes on the DVD's. It is in the old 480p but the quality of the transfer to DVD is dark and about the same quality as your average old VHS tape, not DVD quality. I get the impression Food Network got cheap and subbed out the DVD transfer to the lowest bidder (China?) and it shows.I could watch Giada read the dictionary and her cooking is really first rate. But, that's all you get is watching Giada. Thank god she is easy to understand and her recipes are easy to follow. But to get consistent results on some of the dishes, you should search the web and find the hard copies.  While Giada herself is great, this set is a waste of money. You're better off recording the shows and going to the web to get the recipes you want or, better yet, just stick with her cookbooks which are all first rate and worlds above this cheap presentation.\",\n",
              "       \"I bought these two volumes new and spent over $50 doing so.  You get two DVDs with three discs in each. Why would I buy DVDs if I have the cookbooks? I find watching Ina Garten cook in her kitchen very relaxing after a hectic day.  SHE is doing the cooking, not me.  SHE is having to clean up, not me.  Of course we all know she has a team to do all the prep work and clean up, but we can pretend, can't we? Lots of her earlier episodes with lots of food and friends. Lemon cakes, shrimp dishes, brownies, salads, entrees. I recently made her Szechuan Noodles and the first bowl cost me $40! That's 'cause I didn't have any of the ingredients you need to make it! But once you splurge on all the condiments, you're home free.  I have eaten Szechuan Noodles all week! I can't get enough of her recipe. Hot, cold, warm. They are SO good! That recipe is on the DVD as well. She isn't cutesy like a lot of the Food Network people.  I really dislike that.  Cute doesn't work for me.  I want someone with substance who knows what she's doing. Ina Garten is that gal. You can buy them new, like I did, or used.  Just get them both! You will find yourself in your kitchen at odd hours of the night cooking something! They're addictive. Hope she comes out with Volumes 3, 4 and on.\",\n",
              "       'I am very pleased with the dvd only wish i could get the only volume but unfortunately it cannot be shipped to australia.',\n",
              "       \"I have read many books and literature written on addiction and recovery, and the ones I find the most informative and helpful to those seeking knowledge and understanding are the ones written from individuals who have been there themselves. The school of hard knocks is I believe to be the best education and insights into how and what an addict thinks and acts comes from those who have been there from traditional education. I believe Joe Herzank's personal knowledge and insights. I recommend this DVD to anyone struggling with addiction. Thank God for those like Joe Herzank.\",\n",
              "       \"The information is good, but the presentation is very dry.  I fast-forwarded through a lot of it and wouldn't use it to show to a group as I had intended.\",\n",
              "       \"I have read the original Why Don't They Just Quit and have reviewed the DVD roundtable. I appreciate the talk being so frank and uncomplicated for anyone who is just beginning the &#34;Addiction walk&#34; most specifically as a heartbroken family member. This is a difficult time for everyone. Even if the entire book/DVD doesn't address all of the readers/viewers needs it is well worth the financial investment to buy this product. I was so impressed with its contents, I contributed my own money to the Herzaneks so extra books could be purchased and distributed to those who couldn't afford them.  Heartbroken Gramma Carole\",\n",
              "       'I, too, purchased this to use with my patients in early recovery, an ethnically mixed urban/suburban population who complain that their families, friends,s.o.\\'s \"don\\'t get it\".  If you are purchasing this to INFORM for the first time about substance abuse and why it is difficult to \"just quit\", please choose another way.  This is overly long, meanders, imparts very little useful information, and lacks energy (I\\'m a therapist and it put ME to sleep). I don\\'t think it would find a connection with an audience that is hungry for \"ah-hah\" moments and some insights into their loved one\\'s problem.  It is basically four people (all middle aged Caucasians) sitting in a darkened room around a table asking each other what they think about a range of topics.  It could have been great--personal stories, professional insights, some science, etc., but kind of comes across as a sleepy, low budget afternoon interview show.  I will keep looking.',\n",
              "       \":Why Don't They Just Quit&#34; is a must see for those who have loved ones suffering from the disease of Addiction. Joe answers questions for us that are on this journey of being educated about this terrible disease. We show this DVD at our support group meetings on an ongoing basis.\"],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTjfLO-O2bMM",
        "outputId": "1c9737b1-2c7e-4ce1-cd44-6c5bd926ff4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14.1\n"
          ]
        }
      ],
      "source": [
        "print(torchtext.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the text\n",
        "\n",
        "Here I am converting some basic text in order for model to avoid unnecessary tokens.\n",
        "\n",
        "_for eg: don't --> do not_"
      ],
      "metadata": {
        "id": "np8nQA9KQzcR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M2qwK6lv2bMM"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    import re\n",
        "\n",
        "    text = re.sub(r\"([a-zA-Z]+)n[\\'’]t\", r\"\\1 not\", text)\n",
        "    text = re.sub(r\"([iI])[\\'’]m\", r\"\\1 am\", text)\n",
        "    text = re.sub(r\"([iI])[\\'’]ll\", r\"\\1 will\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\:\\$\\-\\,\\%\\.\\?\\!]+\", \" \", text)\n",
        "    text = html.unescape(text)\n",
        "    # text = re.sub(r\"([a-zA-Z]+)[\\'’]s\", r\"\\1 is\", text)\n",
        "\n",
        "    text = re.sub(r\"_(.*?)_\", r\"\\1\", text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "xOGsoQozNfTG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yruEEKQy2bMM"
      },
      "outputs": [],
      "source": [
        "tokens = []\n",
        "\n",
        "for sent in corpus:\n",
        "   tokens.append(tokenizer(clean_text(sent)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8LOvyZO2bMM",
        "outputId": "77703467-4510-4afb-8a76-cfbbc7528e3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['this',\n",
              "  'has',\n",
              "  'some',\n",
              "  'great',\n",
              "  'tips',\n",
              "  'as',\n",
              "  'always',\n",
              "  'and',\n",
              "  'is',\n",
              "  'helping',\n",
              "  'me',\n",
              "  'to',\n",
              "  'complete',\n",
              "  'my',\n",
              "  'good',\n",
              "  'eats',\n",
              "  'collection',\n",
              "  '.',\n",
              "  'i',\n",
              "  'have',\n",
              "  'not',\n",
              "  'tried',\n",
              "  'any',\n",
              "  'of',\n",
              "  'the',\n",
              "  'recipes',\n",
              "  'yet',\n",
              "  ',',\n",
              "  'but',\n",
              "  'i',\n",
              "  'will',\n",
              "  'soon',\n",
              "  '.',\n",
              "  'sometimes',\n",
              "  'it',\n",
              "  's',\n",
              "  'just',\n",
              "  'lovely',\n",
              "  'to',\n",
              "  'let',\n",
              "  'alton',\n",
              "  'entertain',\n",
              "  'us',\n",
              "  '.'],\n",
              " ['this',\n",
              "  'is',\n",
              "  'a',\n",
              "  'great',\n",
              "  'pastry',\n",
              "  'guide',\n",
              "  '.',\n",
              "  'i',\n",
              "  'love',\n",
              "  'how',\n",
              "  'alton',\n",
              "  's',\n",
              "  'collections',\n",
              "  'can',\n",
              "  'break',\n",
              "  'it',\n",
              "  'down',\n",
              "  'so',\n",
              "  'baking',\n",
              "  'is',\n",
              "  'not',\n",
              "  'so',\n",
              "  'mystical',\n",
              "  'and',\n",
              "  'scary',\n",
              "  '.',\n",
              "  'i',\n",
              "  'might',\n",
              "  'even',\n",
              "  'try',\n",
              "  'some',\n",
              "  'of',\n",
              "  'these',\n",
              "  'recipes',\n",
              "  'some',\n",
              "  'day',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# tokens = np.array(tokens)\n",
        "tokens[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary Preparation\n",
        "\n",
        "Here I am only considering words which are appeared more than 9 times. Otherwise they are considered to be $<unk>$"
      ],
      "metadata": {
        "id": "alJ0XHDLSJA7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "teUzDEnf2bMM"
      },
      "outputs": [],
      "source": [
        "MIN_WORD_FREQUENCY = 10\n",
        "\n",
        "vocab = build_vocab_from_iterator(tokens, min_freq=MIN_WORD_FREQUENCY, specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elpNijLN2bMN",
        "outputId": "ed055a35-416e-49fc-e0a6-4049ca87f49a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'zuzu': 27369,\n",
              " 'zoomed': 27367,\n",
              " 'yucky': 27361,\n",
              " 'yearbook': 27359,\n",
              " 'yamaha': 27357,\n",
              " 'wozniac': 27351,\n",
              " 'worshipping': 27350,\n",
              " 'word-of-mouth': 27349,\n",
              " 'woken': 27346,\n",
              " 'wiseguy': 27344,\n",
              " 'winslett': 27343,\n",
              " 'winningham': 27342,\n",
              " 'windy': 27341,\n",
              " 'wimps': 27340,\n",
              " 'wildness': 27339,\n",
              " 'whooping': 27336,\n",
              " 'whirling': 27334,\n",
              " 'wendell': 27330,\n",
              " 'well-choreographed': 27326,\n",
              " 'warping': 27323,\n",
              " 'wannabes': 27321,\n",
              " 'waned': 27319,\n",
              " 'wallpaper': 27318,\n",
              " 'walk-on': 27316,\n",
              " 'walberg': 27315,\n",
              " 'wah': 27314,\n",
              " 'wackiness': 27313,\n",
              " 'vulnerabilities': 27311,\n",
              " 'voyages': 27310,\n",
              " 'volunteered': 27309,\n",
              " 'violinist': 27308,\n",
              " 'violates': 27306,\n",
              " 'viii': 27305,\n",
              " 'vigilantes': 27304,\n",
              " 'vacations': 27296,\n",
              " 'usurped': 27294,\n",
              " 'upstage': 27293,\n",
              " 'unwillingness': 27291,\n",
              " 'unwed': 27289,\n",
              " 'uninvolving': 27285,\n",
              " 'unheralded': 27284,\n",
              " 'ungodly': 27283,\n",
              " 'unemotional': 27282,\n",
              " 'unearthed': 27281,\n",
              " 'undersea': 27278,\n",
              " 'unconvincingly': 27276,\n",
              " 'unbreakable': 27273,\n",
              " 'unbound': 27272,\n",
              " 'unapologetically': 27271,\n",
              " 'ultimatum': 27270,\n",
              " 'uel': 27267,\n",
              " 'two-faced': 27266,\n",
              " 'twitching': 27265,\n",
              " 'twisters': 27264,\n",
              " 'twelve-year-old': 27263,\n",
              " 'triumphed': 27260,\n",
              " 'tricking': 27258,\n",
              " 'traveller': 27256,\n",
              " 'tr&eacute': 27252,\n",
              " 'townhouse': 27251,\n",
              " 'toothy': 27249,\n",
              " 'toil': 27246,\n",
              " 'toe-tapping': 27245,\n",
              " 'toad': 27244,\n",
              " 'timers': 27243,\n",
              " 'tighten': 27240,\n",
              " 'thrill-ride': 27238,\n",
              " 'three-disc': 27237,\n",
              " 'thicker': 27233,\n",
              " 'thg': 27232,\n",
              " 'terrestrial': 27228,\n",
              " 'tailor-made': 27220,\n",
              " 'synchronized': 27217,\n",
              " 'synchronicity': 27216,\n",
              " 'symmetrical': 27215,\n",
              " 'swung': 27214,\n",
              " 'sweaters': 27209,\n",
              " 'suu': 27207,\n",
              " 'survivalist': 27206,\n",
              " 'suribachi': 27203,\n",
              " 'surfeit': 27201,\n",
              " 'surfacing': 27200,\n",
              " 'sure-fire': 27199,\n",
              " 'sunken': 27198,\n",
              " 'summing': 27197,\n",
              " 'sucky': 27196,\n",
              " 'subtexts': 27194,\n",
              " 'submitting': 27193,\n",
              " 'sub-standard': 27192,\n",
              " 'stun': 27191,\n",
              " 'stump': 27190,\n",
              " 'strobel': 27188,\n",
              " 'strippers': 27187,\n",
              " 'stripe': 27186,\n",
              " 'striker': 27185,\n",
              " 'storied': 27182,\n",
              " 'stomper': 27180,\n",
              " 'stars&quot': 27174,\n",
              " 'squared': 27170,\n",
              " 'squads': 27169,\n",
              " 'springfield': 27168,\n",
              " 'sprays': 27167,\n",
              " 'splashy': 27164,\n",
              " 'spew': 27162,\n",
              " 'spate': 27160,\n",
              " 'spangled': 27159,\n",
              " 'sould': 27157,\n",
              " 'sorority': 27156,\n",
              " 'solaris': 27150,\n",
              " 'soap-opera': 27149,\n",
              " 'sniveling': 27148,\n",
              " 'sniping': 27147,\n",
              " 'snicker': 27146,\n",
              " 'smithereens': 27144,\n",
              " 'slut': 27142,\n",
              " 'slings': 27139,\n",
              " 'sleepwalk': 27137,\n",
              " 'sleepovers': 27136,\n",
              " 'sleepover': 27135,\n",
              " 'skulking': 27134,\n",
              " 'skimp': 27133,\n",
              " 'skelton': 27129,\n",
              " 'simpatico': 27126,\n",
              " 'simonetti': 27125,\n",
              " 'sideline': 27121,\n",
              " 'sickeningly': 27118,\n",
              " 'shrewdly': 27115,\n",
              " 'shootist': 27113,\n",
              " 'shatter': 27110,\n",
              " 'shackles': 27106,\n",
              " 'sh*t': 27105,\n",
              " 'seventh-day': 27104,\n",
              " 'set-piece': 27103,\n",
              " 'serpico': 27102,\n",
              " 'self-sustaining': 27096,\n",
              " 'self-satisfied': 27095,\n",
              " 'self-reliant': 27094,\n",
              " 'seating': 27088,\n",
              " 'sculpt': 27087,\n",
              " 'vicinity': 27303,\n",
              " 'screen&#34': 27086,\n",
              " 'scrapbook': 27082,\n",
              " 'schoolchildren': 27080,\n",
              " 'scenery-chewing': 27078,\n",
              " 'salinger': 27075,\n",
              " 'rubbery': 27070,\n",
              " 'rosary': 27067,\n",
              " 'rory': 27066,\n",
              " 'roasted': 27063,\n",
              " 'roadside': 27062,\n",
              " 'ro': 27061,\n",
              " 'right-on': 27057,\n",
              " 'reynaldo': 27054,\n",
              " 'reverts': 27053,\n",
              " 'retake': 27051,\n",
              " 'resented': 27048,\n",
              " 'reproducing': 27044,\n",
              " 'replicating': 27042,\n",
              " 'replicas': 27041,\n",
              " 'reordered': 27036,\n",
              " 'renter': 27035,\n",
              " 'remorseless': 27034,\n",
              " 'remnant': 27033,\n",
              " 'relly': 27030,\n",
              " 'relieving': 27029,\n",
              " 'reliant': 27028,\n",
              " 'rehearse': 27026,\n",
              " 'regiments': 27024,\n",
              " 'reformatted': 27023,\n",
              " 'redman': 27022,\n",
              " 'recommeded': 27018,\n",
              " 'recomendation': 27017,\n",
              " 'recession': 27016,\n",
              " 'rebuffs': 27014,\n",
              " 'realisticly': 27012,\n",
              " 're-written': 27011,\n",
              " 're-done': 27009,\n",
              " 'raspy': 27006,\n",
              " 'rall': 27005,\n",
              " 'rag-tag': 27002,\n",
              " 'rafferty': 27001,\n",
              " 'raff': 27000,\n",
              " 'rabe': 26998,\n",
              " 'quixotic': 26997,\n",
              " 'quests': 26996,\n",
              " 'quarry': 26993,\n",
              " 'quarantined': 26992,\n",
              " 'qu': 26991,\n",
              " 'purportedly': 26989,\n",
              " 'puritanical': 26988,\n",
              " 'pudge': 26985,\n",
              " 'pseudonym': 26982,\n",
              " 'prominence': 26978,\n",
              " 'programmer': 26977,\n",
              " 'professes': 26975,\n",
              " 'prizes': 26973,\n",
              " 'prix': 26972,\n",
              " 'primera': 26971,\n",
              " 'prickly': 26969,\n",
              " 'prick': 26968,\n",
              " 'prerequisite': 26967,\n",
              " 'prentiss': 26966,\n",
              " 'precipice': 26965,\n",
              " 'preachers': 26964,\n",
              " 'prawer': 26963,\n",
              " 'powerbomb': 26961,\n",
              " 'power-hungry': 26960,\n",
              " 'potted': 26958,\n",
              " 'potipher': 26957,\n",
              " 'postman': 26956,\n",
              " 'poseur': 26954,\n",
              " 'poseidon': 26953,\n",
              " 'portrayel': 26952,\n",
              " 'portayal': 26951,\n",
              " 'pore': 26950,\n",
              " 'pollution': 26947,\n",
              " 'politically-correct': 26946,\n",
              " 'politeness': 26945,\n",
              " 'polemic': 26944,\n",
              " 'polarized': 26943,\n",
              " 'plot-driven': 26942,\n",
              " 'plods': 26941,\n",
              " 'plato': 26939,\n",
              " 'pique': 26937,\n",
              " 'pilgrimage': 26935,\n",
              " 'picturing': 26934,\n",
              " 'physiology': 26933,\n",
              " 'phobia': 26931,\n",
              " 'philly': 26929,\n",
              " 'phantasm': 26928,\n",
              " 'ph': 26927,\n",
              " 'pettiness': 26925,\n",
              " 'perturbed': 26922,\n",
              " 'peppy': 26918,\n",
              " 'pep': 26917,\n",
              " 'peices': 26915,\n",
              " 'peeking': 26914,\n",
              " 'patterson': 26912,\n",
              " 'patrolling': 26911,\n",
              " 'past&quot': 26909,\n",
              " 'passionless': 26908,\n",
              " 'passe': 26907,\n",
              " 'pasolini': 26906,\n",
              " 'particles': 26905,\n",
              " 'parked': 26904,\n",
              " 'pardoned': 26902,\n",
              " 'paragon': 26901,\n",
              " 'pais': 26899,\n",
              " 'paint-by-numbers': 26898,\n",
              " 'painless': 26897,\n",
              " 'ow': 26896,\n",
              " 'overseen': 26894,\n",
              " 'overdid': 26892,\n",
              " 'out-of-print': 26888,\n",
              " 'out-of-control': 26887,\n",
              " 'osama': 26884,\n",
              " 'originaly': 26883,\n",
              " 'ordeals': 26881,\n",
              " 'orchids': 26880,\n",
              " 'opportunism': 26878,\n",
              " 'onion': 26877,\n",
              " 'occupants': 26874,\n",
              " 'obscenely': 26873,\n",
              " 'nussbaum': 26870,\n",
              " 'now-a-days': 26869,\n",
              " 'novelists': 26868,\n",
              " 'noticably': 26867,\n",
              " 'normality': 26865,\n",
              " 'nonchalant': 26864,\n",
              " 'noge': 26861,\n",
              " 'nimble': 26857,\n",
              " 'nest&quot': 26853,\n",
              " 'neglects': 26850,\n",
              " 'negates': 26849,\n",
              " 'nb': 26847,\n",
              " 'naziism': 26846,\n",
              " 'nastiest': 26845,\n",
              " 'narcissist': 26844,\n",
              " 'nab': 26843,\n",
              " 'murmurs': 26842,\n",
              " 'mural': 26841,\n",
              " 'multifaceted': 26840,\n",
              " 'mowing': 26836,\n",
              " 'moulin': 26832,\n",
              " 'motorbike': 26831,\n",
              " 'mother/daughter': 26830,\n",
              " 'morisette': 26828,\n",
              " 'morbidity': 26827,\n",
              " 'mogera': 26824,\n",
              " 'moebius': 26823,\n",
              " 'modification': 26822,\n",
              " 'modernist': 26821,\n",
              " 'moby': 26820,\n",
              " 'mistook': 26819,\n",
              " 'misdirected': 26818,\n",
              " 'mined': 26816,\n",
              " 'mindsets': 26815,\n",
              " 'mimicking': 26813,\n",
              " 'milked': 26812,\n",
              " 'midlife': 26811,\n",
              " 'messege': 26809,\n",
              " 'mesmorizing': 26808,\n",
              " 'mesmorized': 26807,\n",
              " 'mentors': 26806,\n",
              " 'melville': 26805,\n",
              " 'mcguffin': 26801,\n",
              " 'mccourt': 26799,\n",
              " 'mcafee': 26797,\n",
              " 'masquerades': 26793,\n",
              " 'marys': 26791,\n",
              " 'mary&quot': 26790,\n",
              " 'marquess': 26789,\n",
              " 'mandate': 26784,\n",
              " 'man&#8217': 26783,\n",
              " 'mamma': 26781,\n",
              " 'maimed': 26776,\n",
              " 'magistrate': 26774,\n",
              " 'maelstrom': 26773,\n",
              " 'macnichol': 26772,\n",
              " 'mackie': 26771,\n",
              " 'macfadyen': 26770,\n",
              " 'lvc': 26769,\n",
              " 'lulu': 26768,\n",
              " 'lovesick': 26765,\n",
              " 'lounging': 26761,\n",
              " 'loud-mouthed': 26760,\n",
              " 'look-alike': 26757,\n",
              " 'longshot': 26756,\n",
              " 'loath': 26754,\n",
              " 'llyod': 26753,\n",
              " 'listless': 26750,\n",
              " 'lis': 26749,\n",
              " 'lid': 26748,\n",
              " 'lib': 26747,\n",
              " 'lense': 26743,\n",
              " 'leff': 26737,\n",
              " 'layman': 26736,\n",
              " 'latifa': 26735,\n",
              " 'lantz': 26734,\n",
              " 'langencamp': 26732,\n",
              " 'ku': 26728,\n",
              " 'kiev': 26726,\n",
              " 'kietel': 26725,\n",
              " 'kiddos': 26723,\n",
              " 'kid&quot': 26722,\n",
              " 'katyn': 26719,\n",
              " 'kat': 26718,\n",
              " 'kamikaze': 26717,\n",
              " 'juliane': 26715,\n",
              " 'joyless': 26713,\n",
              " 'johnston': 26711,\n",
              " 'jittery': 26709,\n",
              " 'jitterbug': 26708,\n",
              " 'jist': 26707,\n",
              " 'jewishness': 26706,\n",
              " 'jean-jacques': 26705,\n",
              " 'jayne': 26704,\n",
              " 'jars': 26703,\n",
              " 'itis': 26701,\n",
              " 'island&quot': 26698,\n",
              " 'isao': 26697,\n",
              " 'invasions': 26694,\n",
              " 'interrelated': 26688,\n",
              " 'intermingled': 26687,\n",
              " 'inflexible': 26678,\n",
              " 'indisputable': 26674,\n",
              " 'indefinitely': 26673,\n",
              " 'inbred': 26669,\n",
              " 'impersonator': 26667,\n",
              " 'idolatry': 26663,\n",
              " 'idiosyncrasies': 26662,\n",
              " 'hurls': 26659,\n",
              " 'hurdles': 26658,\n",
              " 'human&quot': 26656,\n",
              " 'hq': 26652,\n",
              " 'howler': 26651,\n",
              " 'hotte': 26648,\n",
              " 'hots': 26647,\n",
              " 'hotheaded': 26646,\n",
              " 'hos': 26645,\n",
              " 'hoops': 26643,\n",
              " 'hoop': 26642,\n",
              " 'homicides': 26639,\n",
              " 'home&quot': 26637,\n",
              " 'hisses': 26629,\n",
              " 'hinge': 26628,\n",
              " 'high-profile': 26626,\n",
              " 'hideo': 26625,\n",
              " 'heterosexuals': 26624,\n",
              " 'hesitates': 26623,\n",
              " 'hennessy': 26621,\n",
              " 'heists': 26619,\n",
              " 'hearth': 26618,\n",
              " 'hearse': 26617,\n",
              " 'harms': 26614,\n",
              " 'hard-of-hearing': 26613,\n",
              " 'rikichi': 27058,\n",
              " 'haphazard': 26612,\n",
              " 'handwriting': 26610,\n",
              " 'hands&#34': 26609,\n",
              " 'hammered': 26608,\n",
              " 'hallow': 26607,\n",
              " 'guinea': 26603,\n",
              " 'grumman': 26601,\n",
              " 'grievances': 26597,\n",
              " 'graded': 26595,\n",
              " 'govt': 26594,\n",
              " 'golly': 26590,\n",
              " 'goldmember': 26589,\n",
              " 'godless': 26588,\n",
              " 'godfrey': 26587,\n",
              " 'god-given': 26585,\n",
              " 'glide': 26584,\n",
              " 'giveaway': 26583,\n",
              " 'gigolo': 26582,\n",
              " 'ghoul': 26581,\n",
              " 'gents': 26580,\n",
              " 'genome': 26579,\n",
              " 'genitals': 26578,\n",
              " 'garai': 26575,\n",
              " 'gallon': 26573,\n",
              " 'gagging': 26571,\n",
              " 'fuzz': 26569,\n",
              " 'furnished': 26568,\n",
              " 'funnily': 26567,\n",
              " 'funnies': 26566,\n",
              " 'froth': 26564,\n",
              " 'frivolity': 26563,\n",
              " 'fornication': 26555,\n",
              " 'formatting': 26554,\n",
              " 'foriegn': 26553,\n",
              " 'forgo': 26552,\n",
              " 'footnotes': 26551,\n",
              " 'intertitles': 26689,\n",
              " 'focusses': 26549,\n",
              " 'flown': 26547,\n",
              " 'flies&quot': 26544,\n",
              " 'vanquished': 27298,\n",
              " 'flavored': 26543,\n",
              " 'flatly': 26542,\n",
              " 'flashlight': 26541,\n",
              " 'flashcards': 26540,\n",
              " 'flash-backs': 26539,\n",
              " 'flares': 26538,\n",
              " 'first-person': 26537,\n",
              " 'fireman': 26536,\n",
              " 'firefighter': 26535,\n",
              " 'finn': 26534,\n",
              " 'filmscene': 26533,\n",
              " 'filmgoer': 26532,\n",
              " 'fear&quot': 26523,\n",
              " 'fatality': 26520,\n",
              " 'fantasizes': 26518,\n",
              " 'faltering': 26515,\n",
              " 'falkenberg': 26514,\n",
              " 'facist': 26511,\n",
              " 'f&quot': 26510,\n",
              " 'extravagance': 26508,\n",
              " 'extensions': 26505,\n",
              " 'expulsion': 26504,\n",
              " 'expectant': 26502,\n",
              " 'excursions': 26500,\n",
              " 'excercise': 26498,\n",
              " 'exasperation': 26497,\n",
              " 'exams': 26496,\n",
              " 'ex-lover': 26495,\n",
              " 'evergreen': 26493,\n",
              " 'evaporate': 26492,\n",
              " 'evading': 26491,\n",
              " 'euphoria': 26490,\n",
              " 'espoused': 26489,\n",
              " 'erica': 26488,\n",
              " 'epitomy': 26487,\n",
              " 'envelopes': 26485,\n",
              " 'enumerate': 26484,\n",
              " 'entrances': 26483,\n",
              " 'ensued': 26482,\n",
              " 'engulf': 26479,\n",
              " 'energizing': 26478,\n",
              " 'encapsulated': 26477,\n",
              " 'emphatic': 26473,\n",
              " 'emo': 26472,\n",
              " 'emmit': 26471,\n",
              " 'embellishments': 26470,\n",
              " 'emails': 26469,\n",
              " 'elect': 26467,\n",
              " 'earrings': 26459,\n",
              " 'duress': 26455,\n",
              " 'duos': 26453,\n",
              " 'duning': 26452,\n",
              " 'duff': 26450,\n",
              " 'dues': 26449,\n",
              " 'duds': 26448,\n",
              " 'drunkard': 26446,\n",
              " 'drm': 26443,\n",
              " 'double&quot': 26437,\n",
              " 'doormat': 26435,\n",
              " 'donations': 26433,\n",
              " 'doenitz': 26431,\n",
              " 'dodo': 26430,\n",
              " 'dobbs': 26429,\n",
              " 'disparage': 26420,\n",
              " 'dismissing': 26417,\n",
              " 'disbelieve': 26414,\n",
              " 'disapoint': 26409,\n",
              " 'dinner&quot': 26407,\n",
              " 'digestible': 26406,\n",
              " 'digested': 26405,\n",
              " 'diff': 26404,\n",
              " 'dieing': 26403,\n",
              " 'dictating': 26402,\n",
              " 'diatribes': 26401,\n",
              " 'dew': 26400,\n",
              " 'deutsch': 26399,\n",
              " 'detested': 26398,\n",
              " 'detestable': 26397,\n",
              " 'destroyah': 26395,\n",
              " 'destory': 26394,\n",
              " 'designation': 26391,\n",
              " 'delaney': 26385,\n",
              " 'dekker': 26384,\n",
              " 'decorum': 26380,\n",
              " 'deceptions': 26379,\n",
              " 'decaprio': 26377,\n",
              " 'db': 26373,\n",
              " 'dalby': 26370,\n",
              " 'cynda': 26368,\n",
              " 'cultivated': 26364,\n",
              " 'crusades': 26360,\n",
              " 'crucifying': 26359,\n",
              " 'credulity': 26358,\n",
              " 'crazy&quot': 26357,\n",
              " 'coverup': 26355,\n",
              " 'corrupts': 26352,\n",
              " 'corinthians': 26349,\n",
              " 'cordial': 26348,\n",
              " 'converse': 26347,\n",
              " 'controller': 26346,\n",
              " 'contracting': 26345,\n",
              " 'consistantly': 26343,\n",
              " 'conning': 26341,\n",
              " 'conned': 26340,\n",
              " 'congeniality': 26338,\n",
              " 'off-key': 26875,\n",
              " 'confine': 26336,\n",
              " 'concedes': 26329,\n",
              " 'comraderie': 26328,\n",
              " 'compounded': 26327,\n",
              " 'comply': 26326,\n",
              " 'compendium': 26324,\n",
              " 'commodore': 26322,\n",
              " 'ping': 26936,\n",
              " 'commericals': 26321,\n",
              " 'columns': 26318,\n",
              " 'lemme': 26740,\n",
              " 'collection&quot': 26317,\n",
              " 'colection': 26316,\n",
              " 'cockroaches': 26313,\n",
              " 'coaxes': 26312,\n",
              " 'clutching': 26311,\n",
              " 'clique': 26310,\n",
              " 'clifford': 26309,\n",
              " 'clearing': 26307,\n",
              " 'chute': 26302,\n",
              " 'chronically': 26301,\n",
              " 'chiefs': 26295,\n",
              " 'chefs': 26292,\n",
              " 'cheerfully': 26291,\n",
              " 'checkered': 26290,\n",
              " 'cheapo': 26289,\n",
              " 'cheapen': 26288,\n",
              " 'charred': 26286,\n",
              " 'chaffey': 26283,\n",
              " 'cesspool': 26281,\n",
              " 'cartoon-like': 26277,\n",
              " 'carp': 26276,\n",
              " 'caracter': 26275,\n",
              " 'canes': 26273,\n",
              " 'buy/rent': 26268,\n",
              " 'bungalow': 26266,\n",
              " 'bullwinkle': 26265,\n",
              " 'bull&quot': 26263,\n",
              " 'bulky': 26262,\n",
              " 'buffed': 26258,\n",
              " 'brushing': 26256,\n",
              " 'brushes': 26255,\n",
              " 'kerwin': 26720,\n",
              " 'bruise': 26254,\n",
              " 'brownlow': 26253,\n",
              " 'bront': 26252,\n",
              " 'brogue': 26251,\n",
              " 'brawn': 26247,\n",
              " 'brainwash': 26244,\n",
              " 'bragging': 26242,\n",
              " 'bowman': 26241,\n",
              " 'bowels': 26238,\n",
              " 'bottled': 26234,\n",
              " 'bossy': 26233,\n",
              " 'boothe': 26232,\n",
              " 'bookends': 26231,\n",
              " 'bondarchuk': 26229,\n",
              " 'bloodfest': 26228,\n",
              " 'blondes': 26227,\n",
              " 'bleakly': 26224,\n",
              " 'blanchett': 26223,\n",
              " 'blackness': 26222,\n",
              " 'blackest': 26221,\n",
              " 'biopics': 26220,\n",
              " 'big&quot': 26219,\n",
              " 'zelda': 27366,\n",
              " 'biehn': 26218,\n",
              " 'benevolence': 26217,\n",
              " 'belligerent': 26214,\n",
              " 'beliveable': 26212,\n",
              " 'beheaded': 26209,\n",
              " 'befor': 26207,\n",
              " 'bazooka': 26202,\n",
              " 'bastards': 26198,\n",
              " 'bask': 26197,\n",
              " 'banquine': 26196,\n",
              " 'balked': 26193,\n",
              " 'avenged': 26191,\n",
              " 'automated': 26189,\n",
              " 'autocratic': 26188,\n",
              " 'australians': 26187,\n",
              " 'auspicious': 26186,\n",
              " 'augustine': 26185,\n",
              " 'attuned': 26182,\n",
              " 'atlanta': 26181,\n",
              " 'ashame': 26176,\n",
              " 'asexual': 26175,\n",
              " 'arnez': 26172,\n",
              " 'armand': 26171,\n",
              " 'armacost': 26170,\n",
              " 'arlene': 26169,\n",
              " 'arising': 26168,\n",
              " 'argentinian': 26167,\n",
              " 'arcane': 26166,\n",
              " 'apreciate': 26165,\n",
              " 'apologist': 26162,\n",
              " 'anticipates': 26160,\n",
              " 'anorexia': 26157,\n",
              " 'anointed': 26156,\n",
              " 'ankles': 26155,\n",
              " 'animorphs': 26154,\n",
              " 'angst-ridden': 26153,\n",
              " 'andersen': 26152,\n",
              " 'amplifies': 26150,\n",
              " 'american-made': 26148,\n",
              " 'alvin': 26146,\n",
              " 'aluminum': 26145,\n",
              " 'alignment': 26141,\n",
              " 'alienates': 26140,\n",
              " 'alfalfa': 26139,\n",
              " 'agin': 26134,\n",
              " 'aggravation': 26133,\n",
              " 'aftertaste': 26131,\n",
              " 'advert': 26127,\n",
              " 'administrators': 26126,\n",
              " 'administer': 26124,\n",
              " 'actully': 26120,\n",
              " 'action-thriller': 26119,\n",
              " 'acknowledgement': 26118,\n",
              " 'affirms': 26129,\n",
              " 'accumulation': 26116,\n",
              " 'ac': 26115,\n",
              " 'abolished': 26112,\n",
              " 'abbott': 26111,\n",
              " 'a/k/a': 26110,\n",
              " '`real': 26109,\n",
              " '`boogie': 26106,\n",
              " '^_^': 26105,\n",
              " '8mm&quot': 26102,\n",
              " '60%': 26100,\n",
              " '5-': 26099,\n",
              " '4-stars': 26098,\n",
              " '2d': 26096,\n",
              " '1984&quot': 26093,\n",
              " '1864': 26091,\n",
              " '137': 26087,\n",
              " '135': 26086,\n",
              " '-you': 26083,\n",
              " '-special': 26082,\n",
              " '-5': 26079,\n",
              " 'mclain': 26802,\n",
              " '&#147': 26076,\n",
              " 'ziskey': 26070,\n",
              " 'zerbe': 26069,\n",
              " 'yorkdate': 26063,\n",
              " 'yielding': 26061,\n",
              " 'yearnings': 26059,\n",
              " 'yaz': 26058,\n",
              " 'xena': 26056,\n",
              " 'worshiping': 26054,\n",
              " 'woodstock': 26053,\n",
              " 'woodland': 26052,\n",
              " 'interim': 26686,\n",
              " 'woe': 26051,\n",
              " 'wiz': 26050,\n",
              " 'withstood': 26049,\n",
              " 'wilkie': 26046,\n",
              " 'wiley': 26045,\n",
              " 'wilds': 26044,\n",
              " 'wifes': 26043,\n",
              " 'whe': 26038,\n",
              " 'welling': 26036,\n",
              " 'well-suited': 26035,\n",
              " 'well-educated': 26033,\n",
              " 'well-drawn': 26032,\n",
              " 'well-': 26030,\n",
              " 'washed-out': 26025,\n",
              " 'wales': 26022,\n",
              " 'voskamp': 26018,\n",
              " 'vivacity': 26015,\n",
              " 'visitation': 26014,\n",
              " 'vindication': 26013,\n",
              " 'vibert': 26012,\n",
              " 'veering': 26010,\n",
              " 'upstart': 26008,\n",
              " 'upconverting': 26007,\n",
              " 'unveils': 26005,\n",
              " 'unspeakably': 26004,\n",
              " 'unplug': 26002,\n",
              " 'unneccessary': 26001,\n",
              " 'unloved': 26000,\n",
              " 'unflinchingly': 25997,\n",
              " 'unenjoyable': 25995,\n",
              " 'downsides': 26439,\n",
              " 'undeserved': 25994,\n",
              " 'uncivil': 25990,\n",
              " 'twitchy': 25988,\n",
              " 'tryouts': 25985,\n",
              " 'truthfulness': 25984,\n",
              " 'trashes': 25978,\n",
              " 'leif': 26739,\n",
              " 'transpire': 25976,\n",
              " 'traitors': 25974,\n",
              " 'tokugawa': 25967,\n",
              " 'tivo': 25966,\n",
              " 'titus': 25965,\n",
              " 'bowen': 26239,\n",
              " 'tics': 25964,\n",
              " 'thundering': 25963,\n",
              " 'througout': 25962,\n",
              " 'theoretical': 25955,\n",
              " 'themovie': 25954,\n",
              " 'temperamental': 25950,\n",
              " 'temp': 25949,\n",
              " 'telepathy': 25948,\n",
              " 'telepathically': 25947,\n",
              " 'telegraphed': 25946,\n",
              " 'technicalities': 25945,\n",
              " 'teary-eyed': 25944,\n",
              " 'tatewaki': 25942,\n",
              " 'tandem': 25939,\n",
              " 'tamar': 25938,\n",
              " 'talentless': 25937,\n",
              " 'tac': 25935,\n",
              " 'tabarro': 25934,\n",
              " 'ta': 25933,\n",
              " 't&a': 25932,\n",
              " 'sympathise': 25931,\n",
              " 'swoon': 25929,\n",
              " 'susana': 25928,\n",
              " 'supremacists': 25927,\n",
              " 'summoning': 25924,\n",
              " 'subtler': 25922,\n",
              " 'substituting': 25921,\n",
              " 'subhuman': 25920,\n",
              " 'subconsciously': 25919,\n",
              " 'stuntwork': 25918,\n",
              " 'strident': 25916,\n",
              " 'stolz': 25911,\n",
              " 'stimulates': 25910,\n",
              " 'stickers': 25909,\n",
              " 'steiners': 25907,\n",
              " 'stefano': 25905,\n",
              " 'starve': 25902,\n",
              " 'starr': 25901,\n",
              " 'standoff': 25899,\n",
              " 'stances': 25898,\n",
              " 'stalled': 25897,\n",
              " 'staggeringly': 25896,\n",
              " 'squats': 25894,\n",
              " 'spiegel': 25889,\n",
              " 'spaniards': 25886,\n",
              " 'bled': 26225,\n",
              " 'spaceflight': 25884,\n",
              " 'sock': 25880,\n",
              " 'snoop': 25879,\n",
              " 'smelling': 25877,\n",
              " 'sludge': 25876,\n",
              " 'slovenly': 25874,\n",
              " 'slogan': 25873,\n",
              " 'slid': 25871,\n",
              " 'slice-of-life': 25870,\n",
              " 'sleuth': 25869,\n",
              " 'slender': 25868,\n",
              " 'skirmish': 25865,\n",
              " 'skilfully': 25864,\n",
              " 'skewer': 25863,\n",
              " 'sinless': 25861,\n",
              " 'singlehandedly': 25860,\n",
              " 'sinclair': 25859,\n",
              " 'simulators': 25858,\n",
              " 'siamese': 25855,\n",
              " 'shuffling': 25854,\n",
              " 'fog&quot': 26550,\n",
              " 'sheik': 25851,\n",
              " 'shear': 25850,\n",
              " 'sexless': 25846,\n",
              " 'severly': 25844,\n",
              " 'setups': 25843,\n",
              " 'sergei': 25842,\n",
              " 'sequencing': 25841,\n",
              " 'seperately': 25840,\n",
              " 'sender': 25839,\n",
              " 'combs': 26320,\n",
              " 'self-evident': 25838,\n",
              " 'selection1': 25836,\n",
              " 'seeping': 25835,\n",
              " 'sedgwick': 25834,\n",
              " 'seann': 25831,\n",
              " 'seamen': 25829,\n",
              " 'seafaring': 25828,\n",
              " 'scrolling': 25826,\n",
              " 'scot': 25825,\n",
              " 'scarry': 25823,\n",
              " 'sc': 25821,\n",
              " 'saucer': 25819,\n",
              " 'santo': 25818,\n",
              " 'salena': 25812,\n",
              " 'salacious': 25811,\n",
              " 'saboteurs': 25810,\n",
              " 'rutherford': 25807,\n",
              " 'rumbling': 25806,\n",
              " 'roundtable': 25804,\n",
              " 'roulette': 25803,\n",
              " 'romer': 25802,\n",
              " 'rocksteady': 25797,\n",
              " 'indefinable': 26672,\n",
              " 'rivaling': 25796,\n",
              " 'right&quot': 25795,\n",
              " 'richmond': 25794,\n",
              " 'reveling': 25788,\n",
              " 'retreating': 25787,\n",
              " 'retorts': 25786,\n",
              " 'resolute': 25785,\n",
              " 'resilient': 25784,\n",
              " 'reproach': 25783,\n",
              " 'replicated': 25782,\n",
              " 'repented': 25781,\n",
              " 'repackaged': 25780,\n",
              " 'renewing': 25778,\n",
              " 'ren': 25777,\n",
              " 'relocate': 25775,\n",
              " 'relished': 25774,\n",
              " 'religon': 25773,\n",
              " 'wedlock': 26028,\n",
              " 'relics': 25772,\n",
              " 'reigning': 25771,\n",
              " 'regretting': 25770,\n",
              " 'registry': 25769,\n",
              " 'regaining': 25767,\n",
              " 'redux': 25765,\n",
              " 'rediscovery': 25764,\n",
              " 'receptive': 25761,\n",
              " 'rearranging': 25760,\n",
              " 'real&#34': 25759,\n",
              " 'rdj': 25758,\n",
              " 'ramon': 25755,\n",
              " 'rained': 25753,\n",
              " 'rahzar': 25752,\n",
              " 'quintessentially': 25751,\n",
              " 'pythonesque': 25747,\n",
              " 'prozac': 25739,\n",
              " 'protocol': 25736,\n",
              " 'prophesied': 25734,\n",
              " 'privileges': 25729,\n",
              " 'preschoolers': 25723,\n",
              " 'preschool': 25722,\n",
              " 'pregnancies': 25721,\n",
              " 'predisposed': 25719,\n",
              " 'pre-release': 25718,\n",
              " 'portland': 25714,\n",
              " 'portier': 25713,\n",
              " 'porsche': 25712,\n",
              " 'poppins': 25710,\n",
              " 'popeye': 25709,\n",
              " 'pleasent': 25704,\n",
              " 'planner': 25703,\n",
              " 'pj': 25702,\n",
              " 'pixelated': 25701,\n",
              " 'pixel': 25700,\n",
              " 'pissing': 25699,\n",
              " 'pinhead': 25698,\n",
              " 'piccolino': 25693,\n",
              " 'phoniness': 25692,\n",
              " 'petition': 25690,\n",
              " 'persistently': 25687,\n",
              " 'persevere': 25686,\n",
              " 'perpetuate': 25685,\n",
              " 'periodic': 25684,\n",
              " 'perfunctory': 25683,\n",
              " 'perched': 25681,\n",
              " 'peet': 25680,\n",
              " 'peeling': 25679,\n",
              " 'peeks': 25678,\n",
              " 'payments': 25675,\n",
              " 'pay-off': 25674,\n",
              " 'pastoral': 25673,\n",
              " 'parted': 25672,\n",
              " 'paraphrasing': 25671,\n",
              " 'paramour': 25670,\n",
              " 'pantomime': 25666,\n",
              " 'panorama': 25665,\n",
              " 'pampered': 25663,\n",
              " 'palate': 25662,\n",
              " 'pagent': 25661,\n",
              " 'pared': 26903,\n",
              " 'overturned': 25659,\n",
              " 'oversee': 25656,\n",
              " 'overcast': 25654,\n",
              " 'outnumber': 25651,\n",
              " 'outlawed': 25650,\n",
              " 'cartwright': 26278,\n",
              " 'orgies': 25648,\n",
              " 'organizer': 25646,\n",
              " 'orchestration': 25645,\n",
              " 'orchard': 25644,\n",
              " 'openness': 25640,\n",
              " 'oooh': 25638,\n",
              " 'oldmans': 25632,\n",
              " 'oddbody': 25631,\n",
              " 'od': 25630,\n",
              " 'oblique': 25629,\n",
              " 'notified': 25623,\n",
              " 'norma': 25618,\n",
              " 'non-religious': 25616,\n",
              " 'noh': 25614,\n",
              " 'nobles': 25613,\n",
              " 'night&#34': 25611,\n",
              " 'nickelodeon': 25610,\n",
              " 'neverwhere': 25609,\n",
              " 'near-future': 25607,\n",
              " 'freezer': 26562,\n",
              " 'naught': 25606,\n",
              " 'tabs': 27219,\n",
              " 'natural&#34': 25605,\n",
              " 'nato': 25604,\n",
              " 'narrations': 25603,\n",
              " 'napoleonic': 25602,\n",
              " 'nage': 25600,\n",
              " 'myer': 25599,\n",
              " 'musicologist': 25597,\n",
              " 'mti': 25595,\n",
              " 'movies-': 25594,\n",
              " 'mourns': 25592,\n",
              " 'moths': 25591,\n",
              " 'mos': 25590,\n",
              " 'mods': 25584,\n",
              " 'miyaguchi': 25581,\n",
              " 'mise': 25579,\n",
              " 'misconduct': 25578,\n",
              " 'minh': 25575,\n",
              " 'reverential': 27052,\n",
              " 'minesweeper': 25574,\n",
              " 'milking': 25572,\n",
              " 'militarily': 25571,\n",
              " 'mifume': 25569,\n",
              " 'middle-age': 25568,\n",
              " 'micro-phonies': 25567,\n",
              " 'michaels': 25566,\n",
              " 'methodology': 25565,\n",
              " 'mercer': 25563,\n",
              " 'memphis': 25562,\n",
              " 'meloni': 25561,\n",
              " 'mckeown': 25557,\n",
              " 'mccoy': 25556,\n",
              " 'matter-of-factly': 25551,\n",
              " 'masaki': 25550,\n",
              " 'martyn': 25549,\n",
              " 'martinis': 25548,\n",
              " 'marcy': 25547,\n",
              " 'marbles': 25545,\n",
              " 'manifesto': 25542,\n",
              " 'mag': 25532,\n",
              " 'made-up': 25531,\n",
              " 'made-for-cable': 25530,\n",
              " 'macquitty': 25529,\n",
              " 'lumbering': 25523,\n",
              " 'ferdinand': 26526,\n",
              " 'low-level': 25521,\n",
              " 'love-hate': 25519,\n",
              " 'louisville': 25518,\n",
              " 'lorna': 25517,\n",
              " 'looser': 25515,\n",
              " 'livelihood': 25513,\n",
              " 'litterally': 25512,\n",
              " 'lieutenants': 25510,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "vocab.get_stoi()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGihmw6e2bMN",
        "outputId": "42b3a09d-cf72-454b-d759-1338137068bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in text: 100000\n",
            "Unique words: 27035\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total words in text: {len(tokens)}\")\n",
        "print(f\"Unique words: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "Here I am declaring simple model with just **2 layers**\n",
        "\n",
        "1. Embedding Layer with 300 dimensions as mentioned in the paper that it worked well after trying a lot of dimensions.\n",
        "2. Linear layer that will give back the output as the whole vocabulary.\n",
        "\n",
        "_Here we are only interested in the embedding layer as those are the **featurized representation** of the words._"
      ],
      "metadata": {
        "id": "L5E1k9WXSceN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "rE_wAvTt2bMN"
      },
      "outputs": [],
      "source": [
        "# class CBOW(nn.Module):\n",
        "#     def __init__(self, vocab, embedding_dim, max_norm):\n",
        "#         super(CBOW, self).__init__()\n",
        "#         self.embeddings = nn.Embedding(vocab, embedding_dim, max_norm)\n",
        "#         self.linear = nn.Linear(embedding_dim, vocab)\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         x = self.embeddings(input)\n",
        "#         x = x.mean(axis=1)\n",
        "#         x = self.linear(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "class CBOW2(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, max_norm=5.0):\n",
        "        super(CBOW2, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # self.embeddings = nn.Embedding(vocab_size, embedding_dim, max_norm=max_norm)\n",
        "        # self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_word, output_word, neg_words):\n",
        "        input_emb = self.in_embeddings(input_word)\n",
        "        output_emb = self.out_embeddings(output_word)\n",
        "        neg_emb = self.out_embeddings(neg_words)\n",
        "\n",
        "        print(output_emb.unsqueeze(1).shape, input_emb.shape)\n",
        "        output_loss = torch.bmm(output_emb.unsqueeze(1), input_emb).squeeze().sigmoid().log()\n",
        "        neg_loss = torch.bmm(neg_emb.neg(), input_emb).squeeze().sigmoid().log().sum(1)\n",
        "\n",
        "        return -(output_loss.sum() + neg_loss.sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW_NS_Module(nn.Module):\n",
        "    def __init__(self, emb_size, emb_dims, device):\n",
        "        super(CBOW_NS_Module, self).__init__()\n",
        "        self.device = device\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dims = emb_dims\n",
        "        self.u_embeddings = nn.Embedding(emb_size, emb_dims)\n",
        "        self.v_embeddings = nn.Embedding(emb_size, emb_dims)\n",
        "\n",
        "    def forward(self, src_words, trg_words, wmasks, labels):\n",
        "        p_src_emb = []\n",
        "        for src_word in src_words:\n",
        "            p_src_emb.append(self.u_embeddings(torch.tensor(src_word, dtype=torch.long).to(self.device)).sum(dim=0))\n",
        "        src_emb = torch.stack(p_src_emb)\n",
        "\n",
        "        trg_emb = self.v_embeddings(torch.tensor(trg_words, dtype=torch.long).to(self.device))\n",
        "\n",
        "        wmasks = torch.tensor(wmasks, dtype=torch.float).to(self.device)\n",
        "        labels = torch.tensor(labels, dtype=torch.float).to(self.device)\n",
        "\n",
        "        pred = torch.bmm(src_emb.unsqueeze(1), trg_emb.permute(0, 2, 1)).squeeze(1)\n",
        "\n",
        "        loss = nn.functional.binary_cross_entropy_with_logits(pred.float(), labels, reduction=\"none\", weight=wmasks)\n",
        "        loss = (loss.mean(dim=1) * wmasks.shape[1] / wmasks.sum(dim=1)).mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def get_embeddings(self):\n",
        "        return self.u_embeddings.weight.data.cpu().numpy()"
      ],
      "metadata": {
        "id": "XUVkk9PRqnaG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Dataset\n",
        "\n",
        "- Here I am taking the _window = 4_ i.e. 4 words before and 4 words after to grab the context, as authors mention 3-5 word window works best for the large dataset.\n",
        "\n",
        "- I am also truncating the sequence to maximum of 256 length and creating input-output tensors for them."
      ],
      "metadata": {
        "id": "psc0EXqmTAyM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LeQx2APs2bMN"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "CBOW_WINDOW = 4\n",
        "SEQ_LEN = 256\n",
        "NEG_SAMPLE_SIZE = 4\n",
        "\n",
        "def collate_cbow(batch, text_pipeline, vocab, word_freq):\n",
        "    batch_input, batch_output, batch_neg = [], [], []\n",
        "    batch_src_words, batch_trg_words, wmasks, labels = [], [], [], []\n",
        "    for text in batch:\n",
        "        text_tokens_ids = text_pipeline(text)\n",
        "        if len(text_tokens_ids) < CBOW_WINDOW * 2 + 1:\n",
        "            continue\n",
        "\n",
        "        if SEQ_LEN:\n",
        "            text_tokens_ids = text_tokens_ids[:SEQ_LEN]\n",
        "\n",
        "        # for idx in range(len(text_tokens_ids) - CBOW_WINDOW * 2):\n",
        "        #     token_id_sequence = text_tokens_ids[idx : (idx + CBOW_WINDOW * 2 + 1)]\n",
        "        #     output = token_id_sequence.pop(CBOW_WINDOW)\n",
        "        #     input_ = token_id_sequence\n",
        "        #     batch_input.append(input_)\n",
        "        #     batch_output.append(output)\n",
        "\n",
        "        for idx in range(len(text_tokens_ids) - CBOW_WINDOW * 2):\n",
        "            token_id_sequence = text_tokens_ids[idx : (idx + CBOW_WINDOW * 2 + 1)]\n",
        "\n",
        "            # Taking out the focused target word\n",
        "            output = token_id_sequence.pop(CBOW_WINDOW)\n",
        "\n",
        "            # Rest of the context\n",
        "            input_ = token_id_sequence\n",
        "\n",
        "            neg_samples = []\n",
        "            for j in range(NEG_SAMPLE_SIZE):\n",
        "                rnd_word = random.randint(0, len(vocab) - 1)\n",
        "                while rnd_word in input_:\n",
        "                    rnd_word = random.randint(0, len(vocab) - 1)\n",
        "                neg_samples.append(rnd_word)\n",
        "\n",
        "            batch_src_words += [input_]\n",
        "            batch_trg_words += [[output] + neg_samples]\n",
        "            labels += [[1] + [0] * len(neg_samples)]\n",
        "            wmasks += [[1] * (len(neg_samples)+1)]\n",
        "\n",
        "            batch_input.append(input_)\n",
        "            batch_output.append(output)\n",
        "            batch_neg.append(neg_samples)\n",
        "        \n",
        "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
        "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
        "    batch_neg = torch.tensor(batch_neg, dtype=torch.long)\n",
        "\n",
        "    batch_src_words = torch.tensor(batch_src_words, dtype=torch.long)\n",
        "    batch_trg_words = torch.tensor(batch_trg_words, dtype=torch.long)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    wmasks = torch.tensor(wmasks, dtype=torch.long)\n",
        "\n",
        "\n",
        "    return batch_src_words, batch_trg_words, wmasks, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq = torch.zeros(len(vocab))\n",
        "for word in vocab.get_itos():\n",
        "    word_freq[vocab[word]] = word_freq[vocab[word]] + 1\n",
        "\n",
        "# Normalize word frequencies to create a probability distribution\n",
        "word_freq = word_freq / word_freq.sum()"
      ],
      "metadata": {
        "id": "Wzw-F8LVgpCQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJXcZUT_gsSb",
        "outputId": "f11ff1c8-10ec-46e2-f737-31ed6b561c12"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27035"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HLEodAE6ghRr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Ue7pC_db2bMO"
      },
      "outputs": [],
      "source": [
        "matched_style_corpus = to_map_style_dataset(corpus)\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "train_dataloader = DataLoader(\n",
        "        matched_style_corpus,\n",
        "        batch_size=512,\n",
        "        shuffle=True,\n",
        "        collate_fn=partial(collate_cbow, text_pipeline=text_pipeline, vocab=vocab, word_freq=word_freq),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ljxKtqx2g8O6"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxRIYAOm2bMO",
        "outputId": "85061d2d-ab0b-4b6a-804b-05842bb1ebcc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "196"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_dataloader:\n",
        "  print(i[0].shape, i[1].shape, i[2].shape)\n",
        "  print(i)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SusuuZnoZ5B",
        "outputId": "0ab42653-bc97-42f7-97db-dfadda2fc4fb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([52230, 8]) torch.Size([52230, 5]) torch.Size([52230, 5])\n",
            "(tensor([[   10,   811,     5,  ..., 24205,   112,    39],\n",
            "        [  811,     5,   353,  ...,   112,    39,   349],\n",
            "        [    5,   353,     6,  ...,    39,   349,     0],\n",
            "        ...,\n",
            "        [    7,  6727,     2,  ...,    27,  2844,     1],\n",
            "        [ 6727,     2,  5404,  ...,  2844,     1,   320],\n",
            "        [    2,  5404,    77,  ...,     1,   320,    25]]), tensor([[    6,  9811,   860, 19143,  7329],\n",
            "        [  128, 25369, 18515, 24717, 10186],\n",
            "        [24205, 12849,   140,   904, 14450],\n",
            "        ...,\n",
            "        [   77, 22523,  6310, 25915,  4545],\n",
            "        [  737,  7269,  3947, 10458, 18817],\n",
            "        [   27, 18072, 24406, 13207, 25223]]), tensor([[1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1]]), tensor([[1, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialising Model\n",
        "\n",
        "- Here I am initialising the model, loss function, optimizer, and scheduler."
      ],
      "metadata": {
        "id": "VVp_Rjq3TnHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz7ZsPdKq36E",
        "outputId": "ec15a4bb-50fc-4bbd-abe4-4077972ca61b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "kSimFAzq2bMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8368a480-f510-46f0-df77-637455be9718"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CBOW_NS_Module(\n",
              "  (u_embeddings): Embedding(27035, 300)\n",
              "  (v_embeddings): Embedding(27035, 300)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "EMBED_DIMENSION = 300 \n",
        "EMBED_MAX_NORM = 1 \n",
        "\n",
        "model = CBOW_NS_Module(len(vocab), EMBED_DIMENSION, device)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "dFqQm3V72bMO"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.025)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SqdKYv72bMO",
        "outputId": "18146f8d-4906-4477-dd6a-ecccfd55f713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.5000e-02.\n"
          ]
        }
      ],
      "source": [
        "lr_lambda = lambda epoch: (5 - epoch) / 5\n",
        "lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKUy8gNj2bMO",
        "outputId": "26506c64-97a0-4149-cf2f-2ae80977d12b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/196 [00:00<?, ?it/s]<ipython-input-13-1ada4fe0b2f7>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  p_src_emb.append(self.u_embeddings(torch.tensor(src_word, dtype=torch.long).to(self.device)).sum(dim=0))\n",
            "<ipython-input-13-1ada4fe0b2f7>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  trg_emb = self.v_embeddings(torch.tensor(trg_words, dtype=torch.long).to(self.device))\n",
            "<ipython-input-13-1ada4fe0b2f7>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  wmasks = torch.tensor(wmasks, dtype=torch.float).to(self.device)\n",
            "<ipython-input-13-1ada4fe0b2f7>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels, dtype=torch.float).to(self.device)\n",
            " 13%|█▎        | 26/196 [15:02<1:36:33, 34.08s/it]"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "patience = 5\n",
        "no_progress_num = 0\n",
        "best_epoch_loss = float('inf')\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for mini_batch in tqdm.tqdm(train_dataloader):\n",
        "        batch_loss = model(*mini_batch)\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += batch_loss\n",
        "    print(f\"Epoch {epoch+1}: Loss {epoch_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Section"
      ],
      "metadata": {
        "id": "kNU45UayT1_5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "wGKub9EE2bMP"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOua6ly22bMP",
        "outputId": "f0804ae3-eb29-4a7d-e6bc-44af4ed09088"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CBOW_NS_Module(\n",
              "  (u_embeddings): Embedding(27035, 300)\n",
              "  (v_embeddings): Embedding(27035, 300)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "loss_list = []\n",
        "val_list = []\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "w-q8nAyP2bMP",
        "outputId": "16c4609e-9ecb-465d-ea1f-310a0f9fe237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5681, 1, 300]) torch.Size([5681, 8, 300])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-52f280edc1c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-9bfd1340479f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_word, output_word, neg_words)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0moutput_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mneg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [5681, 300] but got: [5681, 8]."
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = []\n",
        "\n",
        "    for i, batch_data in enumerate(train_dataloader):\n",
        "\n",
        "        inputs = batch_data[0].to(device)\n",
        "        labels = batch_data[1].to(device)\n",
        "        neg_samples = batch_data[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = model(inputs, labels, neg_samples)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss.append(loss.item())\n",
        "        break\n",
        "    \n",
        "    running_loss = np.array(running_loss)\n",
        "    epoch_loss = np.mean(running_loss)\n",
        "    # print(running_loss.shape, epoch_loss)\n",
        "    loss_list.append(epoch_loss)\n",
        "    currloss = loss_list[-1]\n",
        "    print(f\"Epoch: {epoch + 1}/{EPOCHS}, Train Loss={currloss:.5f}, Val Loss={0:.5f}\")\n",
        "\n",
        "    lr_scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the model and Vocabulary"
      ],
      "metadata": {
        "id": "dT2weqbxT6_k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLnYGp-v2bMP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "6ed57e18-b0fa-47fe-e9a0-a5d26fb54a6d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-589c7cd562ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"saved_modelv2.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vocabv2.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "torch.save(model, \"saved_modelv2.pt\")\n",
        "torch.save(vocab, \"vocabv2.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetching the embeddings\n",
        "\n",
        "As discussed above, the embedding layer itself is the ***featurized representation*** of the words."
      ],
      "metadata": {
        "id": "TcPk89h_T9uI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "H3QG_0_-2bMP",
        "outputId": "ee60a1bb-e66c-463a-c99b-97d92e2c9413"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-09a06e0ca3a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# embedding from first model layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# embedding from first model layer\n",
        "embeddings = list(model.parameters())[0]\n",
        "embeddings = embeddings.cpu().detach().numpy()\n",
        "\n",
        "# normalization\n",
        "embed_norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
        "embed_norms = np.reshape(embed_norms, (len(embed_norms), 1))\n",
        "embeddings = embeddings / embed_norms\n",
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG3aYjWo2bMP",
        "outputId": "a8882490-ce0a-4ca4-a289-69a890d23e52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27370"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "tokens = vocab.get_itos()\n",
        "len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffvyVxB_2bMP"
      },
      "outputs": [],
      "source": [
        "def get_top_similar(word, top = 10):\n",
        "    word_id = vocab[word]\n",
        "    if word_id == 0:\n",
        "        print(\"Out of vocabulary word\")\n",
        "        return\n",
        "\n",
        "    word_vec = embeddings[word_id]\n",
        "    word_vec = np.reshape(word_vec, (len(word_vec), 1))\n",
        "    dists = np.matmul(embeddings, word_vec).flatten()\n",
        "    topN_ids = np.argsort(-dists)[1 : top + 1]\n",
        "\n",
        "    topN_dict = {}\n",
        "    for sim_word_id in topN_ids:\n",
        "        sim_word = vocab.lookup_token(sim_word_id)\n",
        "        dist = dists[sim_word_id]\n",
        "        topN_dict[sim_word] = np.round(dist, 3)\n",
        "    return topN_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dagahZkF2bMQ",
        "outputId": "9334fc0b-801c-4b44-88cd-14355d8a9ec3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ship': 0.316,\n",
              " 'holocaust': 0.289,\n",
              " 'moon': 0.279,\n",
              " 'tyburn': 0.271,\n",
              " '1996': 0.265,\n",
              " 'germans': 0.258,\n",
              " '1912': 0.258,\n",
              " 'submarines': 0.256,\n",
              " 'passion': 0.25,\n",
              " 'kiss': 0.248}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "get_top_similar(\"titanic\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B7xnVbDGVJC7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp_learn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}