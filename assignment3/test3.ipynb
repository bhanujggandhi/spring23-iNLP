{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 1], [0, 2, 1], [1, 0, 1], [1, 2, 1], [1, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(26):\n",
    "    temp = []\n",
    "    a = i - 2\n",
    "    b = i - 1\n",
    "    c = i + 1\n",
    "    d = i + 2\n",
    "    temp.extend([a, b, c, d])\n",
    "\n",
    "    # Sub-lists of input and target and 1 as label\n",
    "    for j in range(4):\n",
    "        if temp[j] >= 0 and temp[j] <= 25:\n",
    "            true_list.append([i, temp[j], 1])\n",
    "print(true_list[:5])\n",
    "# [[0, 1, 1], [0, 2, 1], [1, 0, 1], [1, 2, 1], [1, 3, 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# There are 98 true pairs, keeping size 400 selects more than 300 #random pairs or roughly 3 random targets for each input\n",
    "def gen_rand_list(size=400):\n",
    "    # true targets are filtered here as size of vocab is too small\n",
    "    false_list = []\n",
    "    for i in range(size):\n",
    "        frs = random.sample(range(26), 1)[0]\n",
    "        sec = random.sample(range(26), 1)[0]\n",
    "        if abs(frs - sec) > 2 or (frs == sec):\n",
    "            false_list.append([frs, sec, 0])\n",
    "    return false_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_auto():\n",
    "    #Joining and shuffling true and random input/target pairs\n",
    "    joint_list = np.concatenate((np.array(true_list), np.array(gen_rand_list())), axis = 0)\n",
    "    np.random.shuffle(joint_list)\n",
    "    inp_targ_labels = torch.Tensor(joint_list).long()\n",
    "    #Converting both inputs and targets to one-hot forms \n",
    "        #Two tensors are initialized as 0 tensors\n",
    "        #The item in i_th row whose index is equal to corresponding\n",
    "        #input/target is then replaced by 1\n",
    "    middle_word_arr = torch.zeros(inp_targ_labels.shape[0], 26)\n",
    "    sur_word_arr = torch.zeros(inp_targ_labels.shape[0], 26)\n",
    "    for i in range(len(inp_targ_labels)):\n",
    "        middle_word_arr[i, inp_targ_labels[i, 0]] = 1\n",
    "        sur_word_arr[i, inp_targ_labels[i, 1]] = 1\n",
    "    labels = inp_targ_labels[:, 2].float()\n",
    "    return (middle_word_arr, sur_word_arr, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import torch.optim as optim\n",
    "\n",
    "    # Create Network\n",
    "    # 2 fully connected layers with NO bias\n",
    "    # Embedding dimension is 10\n",
    "    # Rows of each weight matrix represents the embedding\n",
    "    # Sigmoid is implemented using loss criterion (nn.BCELoss())\n",
    "    fc_inp_word = nn.Linear(26, 10, bias=False)\n",
    "    fc_targ_word = nn.Linear(26, 10, bias=False)\n",
    "    LR = 0.001\n",
    "    criterion = nn.BCELoss()\n",
    "    params = list(fc_inp_word.parameters()) + list(fc_targ_word.parameters())\n",
    "    optimizer = optim.Adam(params, lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6934)\n",
      "tensor(0.0798)\n",
      "tensor(0.0172)\n",
      "tensor(0.0081)\n",
      "tensor(0.0039)\n",
      "tensor(0.0025)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m dot_u_v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(mid_hot\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(z_midl)):\n\u001b[0;32m---> 17\u001b[0m     dot_u_v[j, :] \u001b[39m=\u001b[39m z_midl[j, :] \u001b[39m@\u001b[39m z_sur[j, :]\n\u001b[1;32m     19\u001b[0m \u001b[39m#Sigmoid activation applied to dot products of vectors\u001b[39;00m\n\u001b[1;32m     20\u001b[0m desired_logits \u001b[39m=\u001b[39m dot_u_v\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "print_every = 1000\n",
    "#In skip-gram middle word becomes the input which predicts #surrounding words(targets)\n",
    "#Every time one_hot_auto() is called fresh batch is generated\n",
    "mid_hot, sur_hot, labels = one_hot_auto()\n",
    "for i in range(epochs):\n",
    "#Forward prop to get hidden layer\n",
    "    z_midl = fc_inp_word(torch.Tensor(mid_hot))\n",
    "    z_sur = fc_targ_word(torch.Tensor(sur_hot))\n",
    "    \n",
    "    #Initialize a 1d matrix of 0s to store dot products between each\n",
    "    #row of first hidden matrix embedding input with second hidden\n",
    "    #matrix embedding target words\n",
    "    #This score forms the basis for optimization\n",
    "    dot_u_v = torch.zeros(mid_hot.shape[0], 1)\n",
    "    for j in range(len(z_midl)):\n",
    "        dot_u_v[j, :] = z_midl[j, :] @ z_sur[j, :]\n",
    "        \n",
    "    #Sigmoid activation applied to dot products of vectors\n",
    "    desired_logits = dot_u_v\n",
    "    sig_logits = nn.Sigmoid()(desired_logits)\n",
    "    \n",
    "    #Back prop and stepping\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(sig_logits, torch.Tensor(labels).view(sig_logits.shape[0], 1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % print_every == 0:\n",
    "        print(loss.data)\n",
    "        #Scheduled one_hot_auto() to generate fresh random pairs \n",
    "        mid_hot, sur_hot, labels = one_hot_auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
